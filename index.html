<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs">
  <meta name="keywords" content="COMPBENCH">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CompBench</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!--<link rel="icon" href="./static/images/favicon.svg">-->
  
  <link rel="icon" href="data:image/svg+xml, <svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚖️</text></svg>">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://heendung.github.io/">Jihyung Kil*</a>,</span>
            <span class="author-block">
              <a href="https://zheda-mai.github.io/">Zheda Mai*</a>,</span>
            <span class="author-block">
              <a href="">Justin Lee</a>,
            <span class="author-block">
              <a href="">Zihe Wang</a>,
            </span>
            <span class="author-block">
              <a href="">Kerrie Cheng</a>,
            </span>            
            <span class="author-block">
              <a href="">Lemeng Wang</a>,
            </span>
            <span class="author-block">
              <a href="">Ye Liu</a>,
            </span>
            <span class="author-block">
              <a href="">Arpita Chowdhury</a>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/wei-lun-harry-chao?pli=1">Wei-Lun Chao</a>
            </span> 
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">The Ohio State University</span>
          </div>

          <div>
            <p style="font-size: 15px !important;">*Equal Contribution</p>
          </div>

          <div>
            <p style="font-size: 15px !important;">{kil.5, mai.145}@osu.edu</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!---
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!---
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              --->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/RaptorMai/CompBenchClean"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://buckeyemailosu-my.sharepoint.com/:f:/g/personal/kil_5_buckeyemail_osu_edu/EjQE4wriHJ1Fm0-Fk3PLuyAB-pyYVqys0eHWRFZwaEymuQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <!-- <section class="section"> -->
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered has-text-centered">-->
      <!-- <div class="column is-four-fifths"> -->
          <img src="static/images/main_fig.jpg">
          <h3 class="subtitle has-text-centered">
            <small>
            <strong>CompBench</strong> offers diverse triplets comprising two images, a question about their relativity, <br> and an answer to cover eight types of relativity.
            </small>
          </h3>
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
    </div>
  </section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The ability to compare objects, scenes, or situations is crucial for effective decision-making and problem-solving in everyday life. 
            For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. 
            Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). 
            In this paper, we introduce CompBench, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMs). 
            CompBench mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. 
            We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. 
            These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. 
            The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. 
            We use CompBench to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. 
            Our results reveal notable shortcomings in their comparative abilities. We believe CompBench not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.
          </p>
          
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2> <br>

        <div class="content has-text-justified">
        <p>
          <strong>CompBench</strong> comprises 39.8K triplets, each containing 1) a pair of visually or semantically relevant images, 2) a question about their relativity, and 3) a ground-truth answer.
          We consider a wide range of questions categorized into eight aspects of relativity. 
        </p>

        <img src="static/images/overall_stat.jpg">

        <p>
          <strong>Attribute Relativity</strong> tests the ability to recognize relative attributes such as size, color, texture, shape, and pattern. 
          For instance, given two images of birds, we ask MLLMs to compare the length of their beaks (e.g., "Which bird has longer beaks?").
          <strong>Existential Relativity</strong> assesses the comprehension of existence in comparisons, asking questions like "Which trait is in the left butterfly but not in the right butterfly?".
          <strong>State/Emotion Relativity</strong> examines if MLLMs can identify state variations, such as different degrees of baking and smiling.
          <strong>Temporal Relativity</strong> evaluates the understanding of time-related changes between two objects or scenes (e.g., "Which video frame happens earlier during a free kick?"). 
          <strong>Spatial Relativity</strong> checks the ability to tell spatial differences (e.g., "Which cup looks further?"). 
          Finally, <strong>Quantitiy/Quality Relativity</strong> investigates whether an MLLM understands the relativity of quantity and quality (e.g., "Which image contains more animal instances?").

        </p>
      </div>
    </div>
    <!--/ Dataset Overview. -->
  </div></section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Curation Pipeline. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Data Curation</h2> <br>
  
          <div class="content has-text-justified">
            <p>
              The data curation pipeline for CompBench includes data selection, question generation, answer annotation, and verification. 
              We rely on combinations of humans, computer programs, MLLMs (specifically GPT-4V), and CLIP similarity to select images and generate questions, based on relativity types and available metadata.
            </p>
  
          </div>
          <img src="static/images/curation_pipeline.jpg">
          <!--
          <h3 class="subtitle has-text-centered">
            <br>
            <small>CompBench Curation Pipeline.</small>
          </h3>-->
          
  
          
        </div>
      </div>
      <!--/ Curation Pipeline. -->
    </div></section>

  
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Results. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2> <br>
  

          <div class="content has-text-justified">
            <p>
              We evaluate four leading MLLMs (i.e., GPT-4V(ision),  Gemini1.0-Pro, LLaVA-1.6,  VILA-1.5) across <strong>eight</strong> relative comparisons spanning <strong>sixteen</strong> tasks.
              The top-performing model in each task is indicated <strong>in bold</strong>. 
              ST: MIT-States,
              FA: Fashionpedia,
              VA: VAW,
              CU: CUB-200-2011,
              WF: Wildfish++,
              MB: MagicBrush,
              SD: Spot-the-diff,
              CE: CelebA,
              FE: FER-2013,
              SN: SoccerNet,
              CC: CompCars,
              ND: NYU-Depth V2,
              VQ: VQAv2,
              QB: Q-Bench2.
            </p>
          </div>
            
          <img src="static/images/results.jpg">
          

          <div class="content has-text-justified">
            <p>
              We observe that current MLLMs face challenges in answering relative questions in CompBench. 
              All MLLMs achieve averaged accuracies over the sixteen tasks (columns) below 80%, with GPT-4V reaching the highest accuracy at 74.7%.
              In particular, the existing MLLMs struggle with answering relative questions related to Existence, Spatiality, and Quantity.
            </p>
          </div>
  
          <div class="content has-text-justified">
            <p>
              <strong>Error Analysis on CompBench</strong>. We observe four types of errors where GPT4-V falls short: 
              (i) differentiating colors between objects and backgrounds, 
              (ii) counting small or distant objects, 
              (iii) identifying objects within crowded scenes, and 
              (iv) recognizing out-of-focus details.
            </p>
          </div>
          
          <img src="static/images/error_analysis.jpg">
          

          
          
        </div>
      </div>
      
      <!--/ Results. -->
    </div>

  </section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      TODO
    </code></pre>
    <!--
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>-->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
